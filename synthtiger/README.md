# Paper Reading
- [SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models](https://arxiv.org/pdf/2107.09313.pdf)
## Introduction
- OCR in the wild consists of two sub-tasks, scene text detection (STD) and scene text recognition (STR). They require similar but different training data. Since STD has to localize text areas from backgrounds, its training example is a raw scene or document snapshot containing multiple texts. In contrast, STR identifies a character sequence from a word box image patch that contains a single word or a line of words. It requires a number of synthetic examples to cover the diversity of styles and texts that might exist in the real world. ***This paper focuses on synthetic data generation for STR to address the diversity of textual appearance in a word box image.***
- In this paper, we introduces a new synthesis engine, referred to as Synthetic Text Image GEneratoR (SynthTIGER), for better STR models.
- ***MJ provides diverse text styles but there is no noise from other texts. The examples of ST are cropped from a scene text image including multiple text boxes and they includes some part of other texts. Although our synthesis engine generates word box images as like MJ, its examples includes text noises observed in examples of ST.***
- ***We propose two methods to alleviate skewed data distribution for infrequent characters and short/lengthy words. Previous synthesis engines generate text images by randomly sampling target texts from a pre-defined lexicon. Due to the low sampling chance of infrequent characters and very short/long words, trained models often poorly perform on these kinds of words. SynthTIGER uses length augmentation and infrequent character augmentation methods to address this problem.***
## Related Works
- There are two popular synthesis engines, MJSynth (MJ) [5] and SynthText (ST) [2].
- [5]
    - By focusing on generating word box images rather than scene text images, MJ can control all text styles, such as font color and size, used in its rendering modules but the generated word box images cannot fully represent text regions cropped from a real scene image.
- [2]
    - There are some constraints on choosing text styles because background regions identified for text rendering may not be compatible with some text rending functions (e.g., too small to use big font size).
- To take advantage of both approaches, recent STR research [19] [1] simply integrates datasets generated by both MJ and ST. However, the simple data integration not only increases the total number of training data but also causes a bias on co-covered data distributions of both synthesis engines. Although the integration provides better STR performance than the individuals, there is still room for improvement by considering a better method combining the benefits of MJ and ST.
## Methodology
- SynthTIGER consists of two major components: text selection and text rendering modules. The text selection module is used to sample a target text, t, from a pre-defined lexicon, L. Then, the text rendering module generates a text image by using multiple fonts F, backgrounds (textures) B, and a color map C.
- Figure 2
    - SynthTIGER renders a target text and a noisy text and combines them to reflect the realness of the text regions (in a wild, a part of a word appearance can be included in a region for another word).SynthTIGER engine consists of five procedures: '(a)' text shape selection, '(b)' text style selection, '(c)' transformation, '(d)' blending, and '(e)' postprocessing. ***The first three processes, '(a)', '(b)', and '(c)', are separately applied to the foreground layer for a target text and the mid-ground layer for a noise text.*** In the '(d)' step, the two layers are combined with a background to represent a single synthesized image. Finally, the '(e)' adds realistic noises.
    - '(a)' Text Shape Selection: Text shape selection decides a 2-dimensional shape of a 1-dimensional character sequence. This process first identify individual character shapes of a target text $t$ and then renders them upon a certain line on 2D space in the left-to-right order. To reveal visual appearances of the characters, ***a font is randomly selected from a pool of font styles*** $F$ ***and each character is rendered upon individual boards with randomly chosen font size and thickness. To add diversity of font styles, elastic distortion [20] is applied to the rendered characters.***
    - Defining a spatial order of characters is essential to map characters upon 2-dimensional space. ***For straight texts, SynthTIGER basically aligns character boards in the left-to-right order with a certain margin between the boards. For curved texts, SynthTIGER places the character boards on a parabolic curve. The curvature of the curve is identified by the maximum height-directional gaps between the centers of the boards. The maximum gap is randomly chosen and the middle points of the target text are allocated on the centroid of the parabolic curve. The character boards upon the curve are rotated with a slope of the curve under a certain probability.***
## References
- [1] [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf)
- [2] [Synthetic Data for Text Localisation in Natural Images](https://arxiv.org/pdf/1604.06646.pdf)
- [5] [Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition](https://arxiv.org/pdf/1406.2227.pdf)
- [19] [ASTER: An Attentional Scene Text Recognizer with Flexible Rectification]
- [20] [Best practices for convolutional neural networks applied to visual document analysis]

# Official Repository
- https://github.com/clovaai/synthtiger
