# Paper Reading
- [SynthTIGER: Synthetic Text Image GEneratoR Towards Better Text Recognition Models](https://arxiv.org/pdf/2107.09313.pdf)
## Introduction
- OCR in the wild consists of two sub-tasks, scene text detection (STD) and scene text recognition (STR). They require similar but different training data. Since STD has to localize text areas from backgrounds, its training example is a raw scene or document snapshot containing multiple texts. In contrast, STR identifies a character sequence from a word box image patch that contains a single word or a line of words. It requires a number of synthetic examples to cover the diversity of styles and texts that might exist in the real world. ***This paper focuses on synthetic data generation for STR to address the diversity of textual appearance in a word box image.***
- In this paper, we introduces a new synthesis engine, referred to as Synthetic Text Image GEneratoR (SynthTIGER), for better STR models.
- ***MJ provides diverse text styles but there is no noise from other texts. The examples of ST are cropped from a scene text image including multiple text boxes and they includes some part of other texts. Although our synthesis engine generates word box images as like MJ, its examples includes text noises observed in examples of ST.***
- ***We propose two methods to alleviate skewed data distribution for infrequent characters and short/lengthy words. Previous synthesis engines generate text images by randomly sampling target texts from a pre-defined lexicon. Due to the low sampling chance of infrequent characters and very short/long words, trained models often poorly perform on these kinds of words. SynthTIGER uses length augmentation and infrequent character augmentation methods to address this problem.***
## Related Works
- There are two popular synthesis engines, MJSynth (MJ) [5] and SynthText (ST) [2].
- [5]
    - By focusing on generating word box images rather than scene text images, MJ can control all text styles, such as font color and size, used in its rendering modules but the generated word box images cannot fully represent text regions cropped from a real scene image.
- [2]
    - There are some constraints on choosing text styles because background regions identified for text rendering may not be compatible with some text rending functions (e.g., too small to use big font size).
- To take advantage of both approaches, recent STR research [19] [1] simply integrates datasets generated by both MJ and ST. However, the simple data integration not only increases the total number of training data but also causes a bias on co-covered data distributions of both synthesis engines. Although the integration provides better STR performance than the individuals, there is still room for improvement by considering a better method combining the benefits of MJ and ST.
## Methodology
- SynthTIGER consists of two major components: text selection and text rendering modules. The text selection module is used to sample a target text, t, from a pre-defined lexicon, L. Then, the text rendering module generates a text image by using multiple fonts F, backgrounds (textures) B, and a color map C.
- Figure 2
    - <img src="https://user-images.githubusercontent.com/67457712/235563806-e772daa4-aab5-404b-afbd-059ea225c245.png" width="600">
    - SynthTIGER renders a target text and a noisy text and combines them to reflect the realness of the text regions (in a wild, a part of a word appearance can be included in a region for another word).SynthTIGER engine consists of five procedures: '(a)' text shape selection, '(b)' text style selection, '(c)' transformation, '(d)' blending, and '(e)' postprocessing. ***The first three processes, '(a)', '(b)', and '(c)', are separately applied to the foreground layer for a target text and the mid-ground layer for a noise text.*** In the '(d)' step, the two layers are combined with a background to represent a single synthesized image. Finally, the '(e)' adds realistic noises.
    - '(a)' Text Shape Selection: Text shape selection decides a 2-dimensional shape of a 1-dimensional character sequence. This process first identify individual character shapes of a target text $t$ and then renders them upon a certain line on 2D space in the left-to-right order. To reveal visual appearances of the characters, ***a font is randomly selected from a pool of font styles*** $F$ ***and each character is rendered upon individual boards with randomly chosen font size and thickness. To add diversity of font styles, elastic distortion [20] is applied to the rendered characters.***
    - Defining a spatial order of characters is essential to map characters upon 2-dimensional space. ***For straight texts, SynthTIGER basically aligns character boards in the left-to-right order with a certain margin between the boards. For curved texts, SynthTIGER places the character boards on a parabolic curve. The curvature of the curve is identified by the maximum height-directional gaps between the centers of the boards. The maximum gap is randomly chosen and the middle points of the target text are allocated on the centroid of the parabolic curve. The character boards upon the curve are rotated with a slope of the curve under a certain probability.***
    - '(b)' Text Style Selection:
        - Text color
            - ***A color map*** $C$ ***is an estimation of a real distribution over colors of text images. It can be identified by clustering colors of real text images. It usually consists of 2, or 3 clusters with the mean gray-scale colors and their standard deviation (s.t.d).***
            - In our work, ***we adapt to the color map used in ST. The color selection from the color map is conducted sequentially in an order of a cluster and a color based on the mean and the s.t.d. Once a color is selected, SynthTIGER changes the color of the character appearances. The colors of texts in the real world is not simply represented with a single color. SynthTIGER uses multiple texture sources,*** $B$ ***, to reflect the realness of text colors.***
        - Text texture
            - ***Specifically, it picks up a random texture from*** $B$ ***, performs a random crop of the texture, and use it as a texture of the text appearance of the synthetic image. In this process, transparency of the texture is also randomly chosen to diversify the effect of textures.**
        - Text boundary style
            - In the real world, the characters’ boundary exhibits diverse patterns depending on text styles, text background, and environmental conditions. ***We can simulate the boundary styles by applying text border, shadow, and extruding effects. SynthTIGER randomly chooses one of these effects and applies it to the text. All required parameters such as effect size and color will be sampled randomly from a pre-defined range.***
    - '(d)' Blending:
        - ***The blending process first creates a background image by randomly sampling color and texture from the colormap*** $C$ ***, and the texture database*** $B$ ***. It randomly changes the transparency of the background texture to diversify the impact of the background. Secondly, it creates two text images, foreground and mid-ground, with the same rendering processes but different random parameters. Foreground contains a target text and mid-ground one carries a noise text. The next step is to combine the mid-ground and background images.*** The blending process first crops the background image to match the text image size. Then, it randomly shifts the noise text in the mid-ground and makes the non-textual area transparent. Finally, ***it merges two images by using one of multiple blending methods: normal, multiply, screen, overlay, hard-light, soft-light, dodge, divide, addition, difference, darken-only, and lighten-only. The last step is to overlay the foreground text image on the merged background.*** The target text area with a little margin is kept non-transparent to distinguish between the target text and the noise text. During this process, it also uses one of the blending methods aforementioned.
<!-- - Table 3
    - Resources
        - Since the outputs of the engines depend on the resources such as fonts, textures, color maps, and a lexicon, we provide fair comparisons, referred as to ""*", by setting the same resources in Table 3. To present a fair comparison, we set the total amount of comparison data as 10M. For example, the total amount of MJ*+ST* is 10M and other comparisons such as MJ*, ST*, and Ours* are also 10M. In Table 3, ours shows clear improvement from single usages of MJ* and ST*. Also, ours have comparable performance with combined datasets. -->
- Resources
    - Table 1
        - Table 1 describes the resources used in MJ and ST as well as in our experiments. As can be seen, MJ and ST are built with their own resources. MJ utilizes a lexicon combining Hunspell corpus4 and ground-truths of real STR examples from ICDAR(IC), SVT, and IIIT datasets. MJ also uses textures and its color map of IC03 and SVT.
        - ST: In contrast, ST does not use the ground-truth information except for the color map from IIIT.
        - SynthTIGER: SynthTIGER utilizes a lexicon consisting of texts of MJ and ST dataset and uses the same textures and color map with ST.
        - "Common*" in the table uses an another lexicon from Wikipedia to evaluate all synthesis engines without ground-truth information of real STR test examples and test sets except for the color map.


Transformation
In detail, SynthTIGER provides stretch, trapezoidate, skew and rotate trans-
formations. Their functions are explained below.
– Stretch adjusts the width or height of the text images.
– Trapezoidate choose an edge of the text image and then adjust its length.
– Skew tilts the text image to one of the four directions such as the right, left,
top and bottom.
– Rotate turns the text image clockwise or anticlockwise.
SynthTIGER applies one of these transformations to the text image with neces-
sary parameter values randomly sampled.
random margins
Finally, SynthTIGER adds random margins to simulate the diverse results of
text detectors. The margins are independently applied to the top, bottom, left,
and right of the image. 

- Visibility
    - A synthesized image created through these steps from (a) to (d) might not be a good text-focused image for several reasons. For example, ***its text and background color happen to be indistinguishable because they are chosen independently. To address this problem, we adopted Flood-Fill algorithm. We apply this algorithm starting from a pixel inside the target text, count the number of text boundary pixels visited, and calculate the ratio of the visited text boundary pixels to the number of all boundary pixels. This process is repeated until all target text pixels are used. If this ratio exceeds a certain threshold, we conclude that the target text and background are indistinguishable and discard the generated image.***
(e) Post-processing
    - ***In this process, SynthTIGER injects general visual noises such as gaussian noise, gaussian blur, resize, median blur and JPEG compression.***

The previous methods, MJ and ST, randomly sample target texts from a user-
provided lexicon. In contrast, SynthTIGER provides two additional strategies to
control the text length distribution and character distribution of a synthesized
dataset. It alleviates the long-tail problem inherited from the use of a lexicon.
Text Length Distribution Control The length distribution of texts ran-
domly sampled from a lexicon does not represent the true distribution of a real
world text data. To alleviate this problem, SynthTIGER performs text length
distribution augmentation with the probability pl.d. where l.d. stands for length
distribution. The augmentation process first randomly chooses the target text
length between 1 and the pre-defined maximum value. Then, it randomly sam-
ples a word from the lexicon. If the word matches the target length, SynthTIGER
uses it as a target text. For longer words, it simply cuts off extra rightmost char-
acters. For shorter words, it samples a new word and attaches it to the right
of the previous one until the concatenated word matches or exceeds the target
length. The rightmost extra characters will be cut off. Text length augmentation,
however, should be used with caution because the generated texts are mostly
nonsensical 

Character Distribution Control Languages such as Chinese and Japanese
use a large number of characters. A synthesized dataset for such a language often
lacks enough amount of samples for rare characters. To deal with this problem,
SynthTIGER conducts character distribution augmentation with the probabil-
ity pc.d where c.d. stands for character distribution. When the augmentation is
triggered, it randomly chooses a character from vocabulary and samples a word
having that character. In the experiments, we show that character distribution
augmentation with pc.d. between 0.25 and 0.5 improves STR performance for
both scene and document domains. 

Experimental Settings for Training and Evaluating STR Models In
this paper, we evaluate synthetic dataset by training a STR model with them
and evaluating the trained model on real STR examples. We choose BEST [1] as
our base model since it is generally used as well as its implementation is publicly
available. All synthetic datasets built for our experiments consists of 10M word
box images. The public datasets, MJ and ST, contains 8.9M and 7M word box
images respectively and they are also evaluated with the same process.
The BEST model are trained only with synthetic datasets. The training and
evaluation is conducted with the STR test-bed6
. Most of experimental settings
follows the training protocol of Baek et al. [1] except for the input image size of
32 by 256.
The evaluation protocol is also the same with [1]. Specifically, we test two
STR scenario depending on languages: one is Latin and the other is Japanese.
For the Latin case, character vocabulary consists of 94 including both alphanu-
meric and special characters. STR models are evaluated on test-sets of STR
benchmarks; 3,000 images of IIIT5k [14], 647 images of SVT [21], 1,110 images
of IC03 [13], 1,095 images of IC13 [7], 2,077 images of IC15 [6], 645 images
of SVTP [17], and 288 images of CUTE80 [18]. We also test performances on
business documents with our in-house 38,493 images. We only evaluate on alpha-
bets and digits due to in-consistent labels of the benchmark datasets. For the
Japanese case, the vocabulary consists of 6,723 characters including alphanu-
meric, special, hiragana/katakana, and some Chinese characters. The evaluation
is conducted on our in-house datasets; 40,938 images of scenes and 38,059 images
of Japanese business documents. 

Comparison on Synthetic Text Data
the
combination of MJ and ST shows better performances than their single usages.
Table 2. Benchmark performances of BEST [1] trained from synthetic text images.
The amount of MJ, ST, MJ+ST, our data are 8.9M, 7M, 15.9M, and 10M, respectively. 

SynthTIGER always provides a better STR performance than the single usages
of MJ and ST. Interestingly, ours achieves comparable or better performance
than combined data (MJ+ST). It should be noted that the amount of combined
training data is 1.5 times larger than ours.
4.3 Comparison on Synthetic Text Image Generators with Same

## References
- [1] [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf)
- [2] [Synthetic Data for Text Localisation in Natural Images](https://arxiv.org/pdf/1604.06646.pdf)
- [5] [Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition](https://arxiv.org/pdf/1406.2227.pdf)
- [19] [ASTER: An Attentional Scene Text Recognizer with Flexible Rectification]
- [20] [Best practices for convolutional neural networks applied to visual document analysis]

# Official Repository
- https://github.com/clovaai/synthtiger
## Configuration
- `weights`: 예를 들어 `[1, 3, 2]`라고 하면 각 확률은 [`1 / 6, 3 / 6, 2 / 6]`이 됩니다. 이 확률 분포를 가지고 각 확률에 해당하는 요소를 샘플링합니다.
## Template
- 템플릿 클래스의 `init_save()`: 생성된 데이터를 저장하기 위한 환경 설정
## Font (`font`):
- `size`:
- `bold`: 볼드 적용 확률
## Color (`colormap2`, `colormap3`)
- "/resources/colormap/iiit5k_gray.txt" 파일의 각 행은 4개 또는 6개의 숫자로 이루어져 있으며 이는 (평균, 표쥰편차)가 2개 또는 3개 연속한 것입니다. ("It usually consists of 2, or 3 clusters with the mean gray-scale colors and their standard deviation (s.t.d).")
- `k`:
    - 2개의 clusters를 가진 컬러를 사용할 지 아니면 3개의 clusters를 가진 컬러를 사용할 지를 의미합니다.
    - 2개의 clusters를 사용한다면 foreground color and background color의 2가지 컬러를 샘플링하는 것이고, 3개의 clusters를 사용한다면 foreground color, background color and style color의 3가지 컬러를 샘플링하는 것입니다.
- `alpha`:
    - RGBA color model의 alpha 값을 샘플링하는 데 사용되며, `alpha[0]` 이상 `alpha[1]` 미만의 실수 중 하나를 uniform distribution에서 샘플링합니다. ("synthtiger/components/color/gray_map.py": `alpha = np.random.uniform(self.alpha[0], self.alpha[1])`)
    - alpha 값이 0일 경우 text border를 제외한 배경이 그대로 비춰 보입니다.
- `colorize`:
    - "synthtiger/utils/image_util.py": `to_rgb()`를 적용할 확률
    - `to_rgb()`를 적용하지 않으면 `rgb = (gry, gray, gray`)를 사용하고, 적용하면 랜덤 샘플링한 RGB 값을 사용합니다.
## Text Style (`style`)
- `prob`: text style 적용 확률
- `class Switch`:
    - `state`: text style의 존재 여부
## Generate Data
- "/images/" 폴더의 하위 폴더에는 최대 10,000 개의 이미지가 저장됩니다. 10,000개가 넘어가면 자동으로 다른 하나의 폴더가 생성됩니다.